{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [第5章: 係り受け解析](https://nlp100.github.io/ja/ch05.html)\n",
    "夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](https://nlp100.github.io/data/neko.txt)）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input/neko.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls Input/neko.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cabocha -f1 Input/neko.txt > Output/Chapter05/neko.txt.cabocha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [40. 係り受け解析結果の読み込み（形態素）](https://nlp100.github.io/ja/ch05.html#40-%E4%BF%82%E3%82%8A%E5%8F%97%E3%81%91%E8%A7%A3%E6%9E%90%E7%B5%90%E6%9E%9C%E3%81%AE%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%81%BF%E5%BD%A2%E6%85%8B%E7%B4%A0)\n",
    "形態素を表すクラス`Morph`を実装せよ．このクラスは表層形（`surface`），基本形（`base`），品詞（`pos`），品詞細分類1（`pos1`）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文を`Morph`オブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morph(object):\n",
    "    def __init__(self,\n",
    "                 surface: str,\n",
    "                 base: str,\n",
    "                 pos: str,\n",
    "                 pos1: str):\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "        \n",
    "    def __str__(self):\n",
    "        args = (self.surface, self.base, self.pos, self.pos1)\n",
    "        return \"surface:%s, base:%s, pos:%s, pos1:%s\" % args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_40 = []\n",
    "\n",
    "with open(\"Output/Chapter05/neko.txt.cabocha\") as neko_cabocha:\n",
    "    sentence = []\n",
    "    for line in neko_cabocha:\n",
    "        if line == \"\\n\" or line == \"　\t記号,空白,*,*,*,*,　,　,　\\n\":\n",
    "            continue\n",
    "        elif line == \"EOS\\n\":\n",
    "            sentences_40.append(sentence)\n",
    "            sentence = []\n",
    "        elif line[0] == \"*\":\n",
    "            # * 文節番号 係り先文節番号(係り先無し=-1) 主辞の形態素番号/機能語の形態素番号 スコア\n",
    "            pass\n",
    "        else:\n",
    "            # 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音\n",
    "            foo = line.split(\"\\t\")\n",
    "            bar = foo[1].split(\",\")\n",
    "            surface = foo[0]\n",
    "            base = \"*\" if \"*\\n\" == bar[6] else bar[6]\n",
    "            pos = bar[0]\n",
    "            pos1 = bar[1]\n",
    "            sentence.append(Morph(surface, base, pos, pos1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface:吾輩, base:吾輩, pos:名詞, pos1:代名詞\n",
      "surface:は, base:は, pos:助詞, pos1:係助詞\n",
      "surface:猫, base:猫, pos:名詞, pos1:一般\n",
      "surface:で, base:だ, pos:助動詞, pos1:*\n",
      "surface:ある, base:ある, pos:助動詞, pos1:*\n",
      "surface:。, base:。, pos:記号, pos1:句点\n"
     ]
    }
   ],
   "source": [
    "for morph in sentences_40[2]:\n",
    "    print(morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [41. 係り受け解析結果の読み込み（文節・係り受け）](https://nlp100.github.io/ja/ch05.html#41-%E4%BF%82%E3%82%8A%E5%8F%97%E3%81%91%E8%A7%A3%E6%9E%90%E7%B5%90%E6%9E%9C%E3%81%AE%E8%AA%AD%E3%81%BF%E8%BE%BC%E3%81%BF%E6%96%87%E7%AF%80%E4%BF%82%E3%82%8A%E5%8F%97%E3%81%91)\n",
    "40に加えて，文節を表すクラス`Chunk`を実装せよ．このクラスは形態素（`Morph`オブジェクト）のリスト（`morphs`），係り先文節インデックス番号（`dst`），係り元文節インデックス番号のリスト（`srcs`）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文を`Chunk`オブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk(object):\n",
    "    def __init__(self,\n",
    "                 morphs: list,\n",
    "                 dst: int,\n",
    "                 srcs: list):\n",
    "        self.morphs = morphs\n",
    "        self.dst = dst\n",
    "        self.srcs = srcs\n",
    "        \n",
    "    def __str__(self):\n",
    "        morphs_str = \"\"\n",
    "        for morph in self.morphs:\n",
    "            morphs_str += morph.surface\n",
    "        args = (morphs_str, self.dst, self.srcs)\n",
    "        return \"morphs:%s, dst:%d, srcs:%s\" % args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "with open(\"Output/Chapter05/neko.txt.cabocha\") as neko_cabocha:\n",
    "    sentence = []\n",
    "    chunk = None\n",
    "    chunk_idx_2_srcs = {}\n",
    "    for line in neko_cabocha:\n",
    "        if line == \"\\n\" or line == \"　\t記号,空白,*,*,*,*,　,　,　\\n\":\n",
    "            continue\n",
    "        elif line == \"EOS\\n\":\n",
    "            if chunk is not None:\n",
    "                sentence.append(chunk)\n",
    "            sentences.append(sentence)\n",
    "            sentence = []\n",
    "            chunk = None\n",
    "            chunk_idx_2_srcs = {}\n",
    "        elif line[0] == \"*\":\n",
    "            if chunk is not None:\n",
    "                sentence.append(chunk)\n",
    "            # * 文節番号 係り先文節番号(係り先無し=-1) 主辞の形態素番号/機能語の形態素番号 スコア\n",
    "            foo = line.split(\" \")\n",
    "            chunk_idx = int(foo[1])\n",
    "            dst = int(foo[2][:-1])\n",
    "            if dst in chunk_idx_2_srcs:\n",
    "                chunk_idx_2_srcs[dst].append(chunk_idx)\n",
    "            else:\n",
    "                chunk_idx_2_srcs[dst] = [chunk_idx]\n",
    "            srcs = [] if chunk_idx not in chunk_idx_2_srcs.keys() else chunk_idx_2_srcs[chunk_idx]\n",
    "            chunk = Chunk([], dst, srcs)\n",
    "        else:\n",
    "            # 表層形\\t品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用型,活用形,原形,読み,発音\n",
    "            foo = line.split(\"\\t\")\n",
    "            bar = foo[1].split(\",\")\n",
    "            surface = foo[0]\n",
    "            base = \"*\" if \"*\\n\" == bar[6] else bar[6]\n",
    "            pos = bar[0]\n",
    "            pos1 = bar[1]\n",
    "            chunk.morphs.append(Morph(surface, base, pos, pos1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "morphs:吾輩は, dst:5, srcs:[]\n",
      "morphs:ここで, dst:2, srcs:[]\n",
      "morphs:始めて, dst:3, srcs:[1]\n",
      "morphs:人間という, dst:4, srcs:[2]\n",
      "morphs:ものを, dst:5, srcs:[3]\n",
      "morphs:見た。, dst:-1, srcs:[0, 4]\n"
     ]
    }
   ],
   "source": [
    "for chunk in sentences[7]:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [42. 係り元と係り先の文節の表示](https://nlp100.github.io/ja/ch05.html#42-%E4%BF%82%E3%82%8A%E5%85%83%E3%81%A8%E4%BF%82%E3%82%8A%E5%85%88%E3%81%AE%E6%96%87%E7%AF%80%E3%81%AE%E8%A1%A8%E7%A4%BA)\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_42 = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    for chunk in sentence:\n",
    "        if chunk.dst == -1:\n",
    "            continue\n",
    "\n",
    "        foo, bar = \"\", \"\"\n",
    "\n",
    "        for morph in chunk.morphs:\n",
    "            foo += morph.surface\n",
    "        foo = foo.replace(\"。\", \"\").replace(\"、\", \"\")\n",
    "\n",
    "        for morph_dst in sentence[chunk.dst].morphs:\n",
    "            bar += morph_dst.surface\n",
    "        bar = bar.replace(\"。\", \"\").replace(\"、\", \"\")\n",
    "\n",
    "        out_42.append(\"%s\\t%s\" % (foo, bar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t猫である\n",
      "吾輩は\t猫である\n",
      "名前は\t無い\n",
      "まだ\t無い\n",
      "どこで\t生れたか\n",
      "生れたか\tつかぬ\n",
      "とんと\tつかぬ\n",
      "見当が\tつかぬ\n",
      "何でも\t薄暗い\n",
      "薄暗い\t所で\n",
      "じめじめした\t所で\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "泣いて\t記憶している\n",
      "いた事だけは\t記憶している\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "始めて\t人間という\n",
      "人間という\tものを\n",
      "ものを\t見た\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(out_42[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [43. 名詞を含む文節が動詞を含む文節に係るものを抽出](https://nlp100.github.io/ja/ch05.html#43-%E5%90%8D%E8%A9%9E%E3%82%92%E5%90%AB%E3%82%80%E6%96%87%E7%AF%80%E3%81%8C%E5%8B%95%E8%A9%9E%E3%82%92%E5%90%AB%E3%82%80%E6%96%87%E7%AF%80%E3%81%AB%E4%BF%82%E3%82%8B%E3%82%82%E3%81%AE%E3%82%92%E6%8A%BD%E5%87%BA)\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_43 = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    for chunk in sentence:\n",
    "        if chunk.dst == -1:\n",
    "            continue\n",
    "\n",
    "        foo, bar = \"\", \"\"\n",
    "\n",
    "        is_skipped = True\n",
    "        for morph in chunk.morphs:\n",
    "            if is_skipped and morph.pos == \"名詞\":\n",
    "                is_skipped = False\n",
    "            foo += morph.surface\n",
    "        foo = foo.replace(\"。\", \"\").replace(\"、\", \"\")\n",
    "\n",
    "        is_skipped = True\n",
    "        for morph_dst in sentence[chunk.dst].morphs:\n",
    "            if is_skipped and morph_dst.pos == \"動詞\":\n",
    "                is_skipped = False\n",
    "            bar += morph_dst.surface\n",
    "        if is_skipped:\n",
    "            continue\n",
    "        bar = bar.replace(\"。\", \"\").replace(\"、\", \"\")\n",
    "\n",
    "        out_43.append(\"%s\\t%s\" % (foo, bar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "どこで\t生れたか\n",
      "生れたか\tつかぬ\n",
      "とんと\tつかぬ\n",
      "見当が\tつかぬ\n",
      "所で\t泣いて\n",
      "ニャーニャー\t泣いて\n",
      "泣いて\t記憶している\n",
      "いた事だけは\t記憶している\n",
      "吾輩は\t見た\n",
      "ここで\t始めて\n",
      "ものを\t見た\n",
      "あとで\t聞くと\n",
      "時々\t捕えて\n",
      "我々を\t捕えて\n",
      "捕えて\t煮て\n",
      "煮て\t食うという\n",
      "しかし\t思わなかった\n",
      "なかったから\t思わなかった\n",
      "恐し\t思わなかった\n",
      "いとも\t思わなかった\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(out_43[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [44. 係り受け木の可視化](https://nlp100.github.io/ja/ch05.html#44-%E4%BF%82%E3%82%8A%E5%8F%97%E3%81%91%E6%9C%A8%E3%81%AE%E5%8F%AF%E8%A6%96%E5%8C%96)\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木を[DOT言語](http://ja.wikipedia.org/wiki/DOT%E8%A8%80%E8%AA%9E)に変換し，[Graphviz](http://www.graphviz.org/)を用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，[pydot](https://code.google.com/p/pydot/)を使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dependency_tree(sentence: list):\n",
    "    digraph = graphviz.Digraph()\n",
    "\n",
    "    for idx, chunk in enumerate(sentence):\n",
    "        label = \"\"\n",
    "        for morph in chunk.morphs:\n",
    "            label += morph.surface\n",
    "        digraph.node(str(idx), label=label)\n",
    "\n",
    "    for idx, chunk in enumerate(sentence):\n",
    "        if chunk.dst == -1:\n",
    "            continue\n",
    "        digraph.edge(str(idx), str(chunk.dst))\n",
    "\n",
    "    digraph.view()\n",
    "\n",
    "    return digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"199pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 198.54 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-328 194.5415,-328 194.5415,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"38.3466\" cy=\"-90\" rx=\"38.1938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"38.3466\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">吾輩は</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"85.3466\" cy=\"-18\" rx=\"38.1938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"85.3466\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">見た。</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;5 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M49.724,-72.5708C55.3816,-63.9038 62.3302,-53.2592 68.5835,-43.6796\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"71.5385,-45.5557 74.074,-35.2687 65.6769,-41.7293 71.5385,-45.5557\"/>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.3466\" cy=\"-306\" rx=\"38.1938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.3466\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ここで</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.3466\" cy=\"-234\" rx=\"38.1938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.3466\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">始めて</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.3466,-287.8314C133.3466,-280.131 133.3466,-270.9743 133.3466,-262.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.8467,-262.4132 133.3466,-252.4133 129.8467,-262.4133 136.8467,-262.4132\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.3466\" cy=\"-162\" rx=\"57.3905\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.3466\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">人間という</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.3466,-215.8314C133.3466,-208.131 133.3466,-198.9743 133.3466,-190.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.8467,-190.4132 133.3466,-180.4133 129.8467,-190.4133 136.8467,-190.4132\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"133.3466\" cy=\"-90\" rx=\"38.1938\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"133.3466\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ものを</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M133.3466,-143.8314C133.3466,-136.131 133.3466,-126.9743 133.3466,-118.4166\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"136.8467,-118.4132 133.3466,-108.4133 129.8467,-118.4133 136.8467,-118.4132\"/>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M121.7271,-72.5708C115.9491,-63.9038 108.8527,-53.2592 102.4663,-43.6796\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"105.3183,-41.6477 96.8591,-35.2687 99.4939,-45.5307 105.3183,-41.6477\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f8c26b6a278>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_dependency_tree(sentences[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [45. 動詞の格パターンの抽出](https://nlp100.github.io/ja/ch05.html#45-%E5%8B%95%E8%A9%9E%E3%81%AE%E6%A0%BC%E3%83%91%E3%82%BF%E3%83%BC%E3%83%B3%E3%81%AE%E6%8A%BD%E5%87%BA)\n",
    "今回用いている文章をコーパスと見なし，日本語の述語が取りうる格を調査したい． 動詞を述語，動詞に係っている文節の助詞を格と考え，述語と格をタブ区切り形式で出力せよ． ただし，出力は以下の仕様を満たすようにせよ．\n",
    "* 動詞を含む文節において，最左の動詞の基本形を述語とする\n",
    "* 述語に係る助詞を格とする\n",
    "* 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "```\n",
    "始める  で\n",
    "見る    は を\n",
    "```\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "* コーパス中で頻出する述語と格パターンの組み合わせ\n",
    "* 「する」「見る」「与える」という動詞の格パターン（コーパス中で出現頻度の高い順に並べよ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output/Chapter05/45.txt\", mode=\"w\") as f:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            line = \"\"\n",
    "\n",
    "            is_skipped = True\n",
    "            for morph in chunk.morphs:\n",
    "                if is_skipped and morph.pos == \"動詞\":\n",
    "                    line += morph.base\n",
    "                    is_skipped = False\n",
    "            if is_skipped:\n",
    "                continue\n",
    "\n",
    "            particles = []\n",
    "            for src in chunk.srcs:\n",
    "                for morph in sentence[src].morphs:\n",
    "                    if morph.pos == \"助詞\":\n",
    "                        particles.append(morph.base)\n",
    "            if len(particles) == 0:\n",
    "                continue\n",
    "\n",
    "            line += \"\\t\"\n",
    "            line += \" \".join(particles)\n",
    "            line += \"\\n\"\n",
    "            line = line.replace(\"。\", \"\").replace(\"、\", \"\")\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    565 云う\tと\r\n",
      "    442 する\tを\r\n",
      "    249 思う\tと\r\n",
      "    199 ある\tが\r\n",
      "    189 なる\tに\r\n",
      "    174 する\tに\r\n",
      "    173 見る\tて\r\n",
      "    127 する\tと\r\n",
      "    117 する\tが\r\n",
      "     94 見る\tを\r\n",
      "     92 見える\tと\r\n",
      "     84 する\tて を\r\n",
      "     60 もつ\tを\r\n",
      "     59 する\tは\r\n",
      "     58 する\tを に\r\n",
      "     58 する\tて\r\n",
      "     56 云う\tを\r\n",
      "     56 ある\tの\r\n",
      "     51 行く\tへ\r\n",
      "     51 する\tが を\r\n"
     ]
    }
   ],
   "source": [
    "!sort Output/Chapter05/45.txt 2>/dev/null | uniq -c | sort -r -k 1 2>/dev/null | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    442 を\r\n",
      "    174 に\r\n",
      "    127 と\r\n",
      "    117 が\r\n",
      "     84 て を\r\n",
      "     59 は\r\n",
      "     58 を に\r\n",
      "     58 て\r\n",
      "     51 が を\r\n",
      "     48 から\r\n",
      "     47 に を\r\n",
      "     42 で を\r\n",
      "     40 の\r\n",
      "     39 も\r\n",
      "     35 と を\r\n",
      "     34 で\r\n",
      "     29 から を\r\n",
      "     28 は を\r\n",
      "     20 は に\r\n",
      "     19 が に\r\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"^する\" Output/Chapter05/45.txt | cut -f 2 | sort | uniq -c | sort -r -k 1 2>/dev/null | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    173 て\r\n",
      "     94 を\r\n",
      "     21 て て\r\n",
      "     20 から\r\n",
      "     16 て を\r\n",
      "     14 と\r\n",
      "     12 で\r\n",
      "     11 は て\r\n",
      "     11 から て\r\n",
      "      8 に\r\n",
      "      7 が を\r\n",
      "      7 が\r\n",
      "      5 は て を\r\n",
      "      4 は て て\r\n",
      "      4 に を\r\n",
      "      4 と を\r\n",
      "      4 で を\r\n",
      "      4 が て\r\n",
      "      3 を に\r\n",
      "      3 も て\r\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"^見る\" Output/Chapter05/45.txt | cut -f 2 | sort | uniq -c | sort -r -k 1 | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3 に を\r\n",
      "      1 ば を\r\n",
      "      1 は て に を に\r\n",
      "      1 は て に を\r\n",
      "      1 に は に対して のみ は も\r\n",
      "      1 に け を\r\n",
      "      1 として を か\r\n",
      "      1 で だけ に を\r\n",
      "      1 て も を\r\n",
      "      1 て は に を\r\n",
      "      1 て に を\r\n",
      "      1 て が は は と て に を\r\n",
      "      1 たり て に を\r\n",
      "      1 じゃあ か と は て を\r\n",
      "      1 けれども に は を\r\n",
      "      1 が を\r\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"^与える\" Output/Chapter05/45.txt | cut -f 2 | sort | uniq -c | sort -r -k 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [46. 動詞の格フレーム情報の抽出](https://nlp100.github.io/ja/ch05.html#46-%E5%8B%95%E8%A9%9E%E3%81%AE%E6%A0%BC%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E6%83%85%E5%A0%B1%E3%81%AE%E6%8A%BD%E5%87%BA)\n",
    "45のプログラムを改変し，述語と格パターンに続けて項（述語に係っている文節そのもの）をタブ区切り形式で出力せよ．45の仕様に加えて，以下の仕様を満たすようにせよ．\n",
    "* 項は述語に係っている文節の単語列とする（末尾の助詞を取り除く必要はない）\n",
    "* 述語に係る文節が複数あるときは，助詞と同一の基準・順序でスペース区切りで並べる\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という例文（neko.txt.cabochaの8文目）を考える． この文は「始める」と「見る」の２つの動詞を含み，「始める」に係る文節は「ここで」，「見る」に係る文節は「吾輩は」と「ものを」と解析された場合は，次のような出力になるはずである．\n",
    "```\n",
    "始める  で      ここで\n",
    "見る    は を   吾輩は ものを\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output/Chapter05/46.txt\", mode=\"w\") as f:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            line = \"\"\n",
    "\n",
    "            is_skipped = True\n",
    "            for morph in chunk.morphs:\n",
    "                if is_skipped and morph.pos == \"動詞\":\n",
    "                    line += morph.base\n",
    "                    is_skipped = False\n",
    "            if is_skipped:\n",
    "                continue\n",
    "\n",
    "            particles, arguments = [], []\n",
    "            for src in chunk.srcs:\n",
    "                particle, argument = None, \"\"\n",
    "                for morph in sentence[src].morphs:\n",
    "                    argument += morph.surface\n",
    "                    if morph.pos == \"助詞\":\n",
    "                        particle = morph.base\n",
    "                if particle is not None:\n",
    "                    particles.append(particle)\n",
    "                    arguments.append(argument)\n",
    "            if len(particles) == 0:\n",
    "                continue\n",
    "\n",
    "            line += \"\\t\"\n",
    "            line += \" \".join(particles)\n",
    "            line += \"\\t\"\n",
    "            line += \" \".join(arguments)\n",
    "            line += \"\\n\"\n",
    "            line = line.replace(\"。\", \"\").replace(\"、\", \"\")\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生れる\tで\tどこで\r\n",
      "つく\tか が\t生れたか 見当が\r\n",
      "泣く\tで\t所で\r\n",
      "する\tて は\t泣いて いた事だけは\r\n",
      "始める\tで\tここで\r\n",
      "見る\tは を\t吾輩は ものを\r\n",
      "聞く\tで\tあとで\r\n",
      "捕える\tを\t我々を\r\n",
      "煮る\tて\t捕えて\r\n",
      "食う\tて\t煮て\r\n",
      "思う\tから\tなかったから\r\n",
      "載せる\tに\t掌に\r\n",
      "持ち上げる\tて と\t載せられて スーと\r\n",
      "ある\tが\t感じが\r\n",
      "落ちつく\tで\t上で\r\n",
      "見る\tて を\t落ちついて 顔を\r\n",
      "見る\tの\tものの\r\n",
      "思う\tと\tものだと\r\n",
      "残る\tが でも\t感じが 今でも\r\n",
      "する\tをもって\t第一毛をもって\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 Output/Chapter05/46.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [47. 機能動詞構文のマイニング](https://nlp100.github.io/ja/ch05.html#47-%E6%A9%9F%E8%83%BD%E5%8B%95%E8%A9%9E%E6%A7%8B%E6%96%87%E3%81%AE%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0)\n",
    "動詞のヲ格にサ変接続名詞が入っている場合のみに着目したい．46のプログラムを以下の仕様を満たすように改変せよ．\n",
    "* 「サ変接続名詞+を（助詞）」で構成される文節が動詞に係る場合のみを対象とする\n",
    "* 述語は「サ変接続名詞+を+動詞の基本形」とし，文節中に複数の動詞があるときは，最左の動詞を用いる\n",
    "* 述語に係る助詞（文節）が複数あるときは，すべての助詞をスペース区切りで辞書順に並べる\n",
    "* 述語に係る文節が複数ある場合は，すべての項をスペース区切りで並べる（助詞の並び順と揃えよ）\n",
    "\n",
    "例えば「別段くるにも及ばんさと、主人は手紙に返事をする。」という文から，以下の出力が得られるはずである．\n",
    "```\n",
    "返事をする      と に は        及ばんさと 手紙に 主人は\n",
    "```\n",
    "このプログラムの出力をファイルに保存し，以下の事項をUNIXコマンドを用いて確認せよ．\n",
    "* コーパス中で頻出する述語（サ変接続名詞+を+動詞）\n",
    "* コーパス中で頻出する述語と助詞パターン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output/Chapter05/47.txt\", mode=\"w\") as f:\n",
    "    for sentence in sentences:\n",
    "        sahen_idx, sahen_noun, wo = None, \"\", \"\"\n",
    "        for idx, chunk in enumerate(sentence):\n",
    "            line = \"\"\n",
    "\n",
    "            for morph in chunk.morphs:\n",
    "                if morph.pos1 == \"サ変接続\":\n",
    "                    sahen_idx, sahen_noun, wo = idx, morph.base, \"\"\n",
    "                elif sahen_noun != \"\" and morph.base == \"を\":\n",
    "                    wo = morph.base\n",
    "                elif sahen_noun != \"\" and wo != \"\" and morph.pos == \"動詞\":\n",
    "                    line = morph.base\n",
    "                    break\n",
    "                else:\n",
    "                    sahen_noun, wo = \"\", \"\"\n",
    "            if sahen_noun != \"\" and wo != \"\" and line != \"\":\n",
    "                line = sahen_noun + wo + line\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            particles, arguments = [], []\n",
    "            for src in chunk.srcs:\n",
    "                if src >= sahen_idx:\n",
    "                    continue\n",
    "                particle, argument = None, \"\"\n",
    "                for morph in sentence[src].morphs:\n",
    "                    argument += morph.surface\n",
    "                    if morph.pos == \"助詞\":\n",
    "                        particle = morph.base\n",
    "                if particle is not None:\n",
    "                    particles.append(particle)\n",
    "                    arguments.append(argument)\n",
    "            if len(particles) == 0:\n",
    "                continue\n",
    "\n",
    "            line += \"\\t\"\n",
    "            line += \" \".join(particles)\n",
    "            line += \"\\t\"\n",
    "            line += \" \".join(arguments)\n",
    "            line += \"\\n\"\n",
    "            line = line.replace(\"。\", \"\").replace(\"、\", \"\")\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     26 返事をする\r\n",
      "     19 挨拶をする\r\n",
      "     10 話をする\r\n",
      "      7 喧嘩をする\r\n",
      "      6 真似をする\r\n",
      "      5 質問をする\r\n",
      "      5 質問をかける\r\n",
      "      5 相談をする\r\n",
      "      5 昼寝をする\r\n",
      "      4 降参をする\r\n",
      "      4 辞儀をする\r\n",
      "      4 演説をする\r\n",
      "      4 注意をする\r\n",
      "      4 欠伸をする\r\n",
      "      4 休養を要する\r\n",
      "      3 講釈をする\r\n",
      "      3 落着を告げる\r\n",
      "      3 病気をする\r\n",
      "      3 活躍を試みる\r\n",
      "      3 決心をする\r\n"
     ]
    }
   ],
   "source": [
    "!cut -f 1 Output/Chapter05/47.txt | sort | uniq -c | sort -r -k 1 2>/dev/null | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6 と\r\n",
      "      3 は と\r\n",
      "      1 も\r\n",
      "      1 へ\r\n",
      "      1 は\r\n",
      "      1 と は に\r\n",
      "      1 と は で\r\n",
      "      1 と は\r\n",
      "      1 と に\r\n",
      "      1 と と は に\r\n",
      "      1 と と は と\r\n",
      "      1 と が から に\r\n",
      "      1 と が\r\n",
      "      1 と から\r\n",
      "      1 て と\r\n",
      "      1 て\r\n",
      "      1 から と\r\n",
      "      1 から て で\r\n",
      "      1 から て\r\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"^返事をする\" Output/Chapter05/47.txt | cut -f 2 | sort | uniq -c | sort -r -k 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4 と\r\n",
      "      4 から\r\n",
      "      2 と も\r\n",
      "      2 で\r\n",
      "      1 は て と\r\n",
      "      1 ので て\r\n",
      "      1 と は によって の\r\n",
      "      1 と は で\r\n",
      "      1 と は\r\n",
      "      1 て\r\n",
      "      1 が て と\r\n"
     ]
    }
   ],
   "source": [
    "!grep -E \"^挨拶をする\" Output/Chapter05/47.txt | cut -f 2 | sort | uniq -c | sort -r -k 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [48. 名詞から根へのパスの抽出](https://nlp100.github.io/ja/ch05.html#48-%E5%90%8D%E8%A9%9E%E3%81%8B%E3%82%89%E6%A0%B9%E3%81%B8%E3%81%AE%E3%83%91%E3%82%B9%E3%81%AE%E6%8A%BD%E5%87%BA)\n",
    "文中のすべての名詞を含む文節に対し，その文節から構文木の根に至るパスを抽出せよ． ただし，構文木上のパスは以下の仕様を満たすものとする．\n",
    "* 各文節は（表層形の）形態素列で表現する\n",
    "* パスの開始文節から終了文節に至るまで，各文節の表現を\"`->`\"で連結する\n",
    "\n",
    "「吾輩はここで始めて人間というものを見た」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "```\n",
    "吾輩は -> 見た\n",
    "ここで -> 始めて -> 人間という -> ものを -> 見た\n",
    "人間という -> ものを -> 見た\n",
    "ものを -> 見た\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output/Chapter05/48.txt\", mode=\"w\") as f:\n",
    "    for sentence in sentences:\n",
    "        for chunk in sentence:\n",
    "            root, is_skipped = \"\", True\n",
    "            for morph in chunk.morphs:\n",
    "                root += morph.surface\n",
    "                if morph.pos == \"名詞\":\n",
    "                    is_skipped = False\n",
    "            if is_skipped:\n",
    "                continue\n",
    "\n",
    "            def _stretch_line(dst: int):\n",
    "                if dst == -1:\n",
    "                    return \"\"\n",
    "                else:\n",
    "                    chunk_dst, node = sentence[dst], \"\"\n",
    "                    for morph in chunk_dst.morphs:\n",
    "                        node += morph.surface\n",
    "                    return \" -> \" + node + _stretch_line(chunk_dst.dst)\n",
    "\n",
    "            line = root\n",
    "            line += _stretch_line(chunk.dst)\n",
    "            line += \"\\n\"\n",
    "            line = line.replace(\"。\", \"\").replace(\"、\", \"\")\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一\r\n",
      "吾輩は -> 猫である\r\n",
      "猫である\r\n",
      "名前は -> 無い\r\n",
      "どこで -> 生れたか -> つかぬ\r\n",
      "見当が -> つかぬ\r\n",
      "何でも -> 薄暗い -> 所で -> 泣いて -> 記憶している\r\n",
      "所で -> 泣いて -> 記憶している\r\n",
      "ニャーニャー -> 泣いて -> 記憶している\r\n",
      "いた事だけは -> 記憶している\r\n",
      "記憶している\r\n",
      "吾輩は -> 見た\r\n",
      "ここで -> 始めて -> 人間という -> ものを -> 見た\r\n",
      "人間という -> ものを -> 見た\r\n",
      "ものを -> 見た\r\n",
      "あとで -> 聞くと -> 種族であったそうだ\r\n",
      "それは -> 種族であったそうだ\r\n",
      "書生という -> 人間中で -> 種族であったそうだ\r\n",
      "人間中で -> 種族であったそうだ\r\n",
      "一番 -> 獰悪な -> 種族であったそうだ\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 Output/Chapter05/48.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [49. 名詞間の係り受けパスの抽出](https://nlp100.github.io/ja/ch05.html#49-%E5%90%8D%E8%A9%9E%E9%96%93%E3%81%AE%E4%BF%82%E3%82%8A%E5%8F%97%E3%81%91%E3%83%91%E3%82%B9%E3%81%AE%E6%8A%BD%E5%87%BA)\n",
    "文中のすべての名詞句のペアを結ぶ最短係り受けパスを抽出せよ．ただし，名詞句ペアの文節番号が$i$と$j$（$i<j$）のとき，係り受けパスは以下の仕様を満たすものとする．\n",
    "* 問題48と同様に，パスは開始文節から終了文節に至るまでの各文節の表現（表層形の形態素列）を\"`->`\"で連結して表現する\n",
    "* 文節$i$と$j$に含まれる名詞句はそれぞれ，XとYに置換する\n",
    "\n",
    "また，係り受けパスの形状は，以下の2通りが考えられる．\n",
    "* 文節$i$から構文木の根に至る経路上に文節$j$が存在する場合: 文節$i$から文節$j$のパスを表示\n",
    "* 上記以外で，文節$i$と文節$j$から構文木の根に至る経路上で共通の文節$k$で交わる場合: 文節$i$から文節$k$に至る直前のパスと文節$j$から文節$k$に至る直前までのパス，文節kの内容を\"`|`\"で連結して表示\n",
    "\n",
    "例えば，「吾輩はここで始めて人間というものを見た。」という文（neko.txt.cabochaの8文目）から，次のような出力が得られるはずである．\n",
    "```\n",
    "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\n",
    "Xは | Yという -> ものを | 見た\n",
    "Xは | Yを | 見た\n",
    "Xで -> 始めて -> Y\n",
    "Xで -> 始めて -> 人間という -> Y\n",
    "Xという -> Y\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Output/Chapter05/49.txt\", mode=\"w\") as f:\n",
    "    for sentence in sentences:\n",
    "        for i in range(len(sentence)-1):\n",
    "            chunk_i = sentence[i]\n",
    "            node_i, is_skipped = \"\", True\n",
    "            for morph in chunk_i.morphs:\n",
    "                if morph.pos == \"名詞\":\n",
    "                    node_i += \"X\"\n",
    "                    is_skipped = False\n",
    "                else:\n",
    "                    node_i += morph.surface\n",
    "            if is_skipped:\n",
    "                continue\n",
    "\n",
    "            path_i = {i: node_i}\n",
    "            def _stretch_path(path: dict, dst: int):\n",
    "                if dst != -1:\n",
    "                    chunk_dst, node = sentence[dst], \"\"\n",
    "                    for morph in chunk_dst.morphs:\n",
    "                        node += morph.surface\n",
    "                    path[dst] = node\n",
    "                    _stretch_path(path, sentence[dst].dst)\n",
    "            _stretch_path(path_i, chunk_i.dst)\n",
    "\n",
    "            for j in range(i+1, len(sentence)):\n",
    "                chunk_j = sentence[j]\n",
    "                node_j, is_skipped = \"\", True\n",
    "                for morph in chunk_j.morphs:\n",
    "                    if morph.pos == \"名詞\":\n",
    "                        node_j += \"Y\"\n",
    "                        is_skipped = False\n",
    "                    else:\n",
    "                        node_j += morph.surface\n",
    "                if is_skipped:\n",
    "                    continue\n",
    "\n",
    "                path_j = {j: node_j}\n",
    "                _stretch_path(path_j, chunk_j.dst)\n",
    "\n",
    "                line = node_i\n",
    "                for k, node_k in path_i.items():\n",
    "                    if k == i:\n",
    "                        continue\n",
    "                    if j in path_i.keys() and k == j:\n",
    "                        line += \" -> \" + node_j\n",
    "                        break\n",
    "                    elif j not in path_i.keys() and k in path_j.keys():\n",
    "                        line += \" | \" + node_j\n",
    "                        for l, node_l in path_j.items():\n",
    "                            if l != j and l != k:\n",
    "                                line += \" -> \" + node_l\n",
    "                        line += \" | \" + node_k\n",
    "                    else:\n",
    "                        line += \" -> \" + node_k\n",
    "\n",
    "                line += \"\\n\"\n",
    "                line = line.replace(\"。\", \"\").replace(\"、\", \"\")\n",
    "                f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xは -> Yである\r\n",
      "Xで -> 生れたか | Yが | つかぬ\r\n",
      "Xでも -> 薄暗い -> Yで\r\n",
      "Xでも -> 薄暗い -> 所で | Y -> 記憶している | 泣いて | Y -> 泣いて | 記憶している\r\n",
      "Xでも -> 薄暗い -> 所で -> 泣いて | Yだけは | 記憶している\r\n",
      "Xでも -> 薄暗い -> 所で -> 泣いて -> Yしている\r\n",
      "Xで | Y -> 記憶している | 泣いて | Y -> 泣いて | 記憶している\r\n",
      "Xで -> 泣いて | Yだけは | 記憶している\r\n",
      "Xで -> 泣いて -> Yしている\r\n",
      "X -> 泣いて | Yだけは | 記憶している\r\n",
      "X -> 泣いて -> Yしている\r\n",
      "Xだけは -> Yしている\r\n",
      "Xは | Yで -> 始めて -> 人間という -> ものを | 見た\r\n",
      "Xは | Yという -> ものを | 見た\r\n",
      "Xは | Yを | 見た\r\n",
      "Xで -> 始めて -> Yという\r\n",
      "Xで -> 始めて -> 人間という -> Yを\r\n",
      "Xという -> Yを\r\n",
      "Xで -> 聞くと | Yは | 種族であったそうだ\r\n",
      "Xで -> 聞くと | Yという -> 人間中で | 種族であったそうだ\r\n"
     ]
    }
   ],
   "source": [
    "!head -20 Output/Chapter05/49.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
