{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "67ee809a-b00f-43be-81b5-9e3ed6011e4b"
    }
   },
   "source": [
    "# [第8章: ニューラルネット](https://nlp100.github.io/ja/ch08.html)\n",
    "第6章で取り組んだニュース記事のカテゴリ分類を題材として，ニューラルネットワークでカテゴリ分類モデルを実装する．なお，この章ではPyTorch, TensorFlow, Chainerなどの機械学習プラットフォームを活用せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "905a6071-fafd-465b-9b5d-c46f64f1db2e"
    }
   },
   "source": [
    "## [70. 単語ベクトルの和による特徴量](https://nlp100.github.io/ja/ch08.html#70-%E5%8D%98%E8%AA%9E%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%81%AE%E5%92%8C%E3%81%AB%E3%82%88%E3%82%8B%E7%89%B9%E5%BE%B4%E9%87%8F)\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例$x_i$の特徴ベクトル$\\boldsymbol{x}_i$を並べた行列$X$と，正解ラベルを並べた行列（ベクトル）$Y$を作成したい．\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix} \n",
    "  \\boldsymbol{x}_1 \\\\ \n",
    "  \\boldsymbol{x}_2 \\\\ \n",
    "  \\dots \\\\ \n",
    "  \\boldsymbol{x}_n \\\\ \n",
    "\\end{pmatrix} \\in \\mathbb{R}^{n \\times d},\n",
    "Y = \\begin{pmatrix} \n",
    "  y_1 \\\\ \n",
    "  y_2 \\\\ \n",
    "  \\dots \\\\ \n",
    "  y_n \\\\ \n",
    "\\end{pmatrix} \\in \\mathbb{N}^{n}\n",
    "$$\n",
    "\n",
    "ここで，$n$は学習データの事例数であり，$\\boldsymbol{x}_i \\in \\mathbb{R}^d$と$y_i \\in \\mathbb{N}$はそれぞれ，$i \\in \\{1, \\dots, n\\}$番目の事例の特徴量ベクトルと正解ラベルを表す．\n",
    "なお，今回は「ビジネス」「科学技術」「エンターテイメント」「健康」の4カテゴリ分類である．$\\mathbb{N}_{<4}$で$4$未満の自然数（$0$を含む）を表すことにすれば，任意の事例の正解ラベル$y_i$は$y_i \\in \\mathbb{N}_{<4}$で表現できる．\n",
    "以降では，ラベルの種類数を$L$で表す（今回の分類タスクでは$L=4$である）．\n",
    "\n",
    "$i$番目の事例の特徴ベクトル$\\boldsymbol{x}_i$は，次式で求める．\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_i = \\frac{1}{T_i} \\sum_{t=1}^{T_i} \\mathrm{emb}(w_{i,t})\n",
    "$$\n",
    "\n",
    "ここで，$i$番目の事例は$T_i$個の（記事見出しの）単語列$(w_{i,1}, w_{i,2}, \\dots, w_{i,T_i})$から構成され，$\\mathrm{emb}(w) \\in \\mathbb{R}^d$は単語$w$に対応する単語ベクトル（次元数は$d$）である．すなわち，$i$番目の事例の記事見出しを，その見出しに含まれる単語のベクトルの平均で表現したものが$\\boldsymbol{x}_i$である．今回は単語ベクトルとして，問題60でダウンロードしたものを用いればよい．$300$次元の単語ベクトルを用いたので，$d=300$である．\n",
    "\n",
    "$i$番目の事例のラベル$y_i$は，次のように定義する．\n",
    "\n",
    "$$\n",
    "y_i = \\begin{cases}\n",
    "0 & (\\mbox{記事}x_i\\mbox{が「ビジネス」カテゴリの場合}) \\\\\n",
    "1 & (\\mbox{記事}x_i\\mbox{が「科学技術」カテゴリの場合}) \\\\\n",
    "2 & (\\mbox{記事}x_i\\mbox{が「エンターテイメント」カテゴリの場合}) \\\\\n",
    "3 & (\\mbox{記事}x_i\\mbox{が「健康」カテゴリの場合}) \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "なお，カテゴリ名とラベルの番号が一対一で対応付いていれば，上式の通りの対応付けでなくてもよい．\n",
    "\n",
    "以上の仕様に基づき，以下の行列・ベクトルを作成し，ファイルに保存せよ．\n",
    "* 学習データの特徴量行列: $X_{\\rm train} \\in \\mathbb{R}^{N_t \\times d}$\n",
    "* 学習データのラベルベクトル: $Y_{\\rm train} \\in \\mathbb{N}^{N_t}$\n",
    "* 検証データの特徴量行列: $X_{\\rm valid} \\in \\mathbb{R}^{N_v \\times d}$\n",
    "* 検証データのラベルベクトル: $Y_{\\rm valid} \\in \\mathbb{N}^{N_v}$\n",
    "* 評価データの特徴量行列: $X_{\\rm test} \\in \\mathbb{R}^{N_e \\times d}$\n",
    "* 評価データのラベルベクトル: $Y_{\\rm test} \\in \\mathbb{N}^{N_e}$\n",
    "\n",
    "なお，$N_t, N_v, N_e$はそれぞれ，学習データの事例数，検証データの事例数，評価データの事例数である．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "12431ef1-77f5-4980-a274-ec49cd6a3e4f"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input/GoogleNews-vectors-negative300.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls Input/GoogleNews-vectors-negative300.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "aac521ed-8826-46c4-9bfd-9264a1f8d9bf"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output/Chapter06/test.txt\r\n",
      "Output/Chapter06/train.txt\r\n",
      "Output/Chapter06/valid.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls Output/Chapter06/*.txt | grep -v feature.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "b41d8c6c-db1f-45ec-83e1-dabe516a430b"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "5d5bc52a-8d96-4b8e-831e-6d59952197a3"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.3 s, sys: 5.91 s, total: 26.2 s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "wv = KeyedVectors.load_word2vec_format(\"Input/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "labels = [\"b\", \"t\", \"e\", \"m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "445eabad-a22b-4ca2-927f-902a1de53615"
    }
   },
   "outputs": [],
   "source": [
    "def save_features(file_type: str):\n",
    "    if file_type != \"train\" and file_type != \"valid\" and file_type != \"test\":\n",
    "        raise ValueError(\"file_type:%s\" % file_type)\n",
    "\n",
    "    in_path = \"Output/Chapter06/%s.txt\" % file_type\n",
    "    out_paths = (\"Output/Chapter08/x_%s.npy\" % file_type,\n",
    "                 \"Output/Chapter08/y_%s.npy\" % file_type)\n",
    "    x, y = [], []\n",
    "\n",
    "    with open(in_path) as f:\n",
    "        for line in f:\n",
    "            elements = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            label, words = elements[0], elements[1].split()\n",
    "            wvs = [wv[word] if word in wv else np.zeros(300) for word in words]\n",
    "            x.append(sum(wvs) / len(words))\n",
    "            y.append(labels.index(label))\n",
    "\n",
    "    np.save(out_paths[0], np.array(x))\n",
    "    np.save(out_paths[1], np.array(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 616 ms, sys: 44 ms, total: 660 ms\n",
      "Wall time: 665 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_features(\"train\")\n",
    "save_features(\"valid\")\n",
    "save_features(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f6869d9e-85c9-4a2b-a04e-4f80ccafd356"
    }
   },
   "source": [
    "## [71. 単層ニューラルネットワークによる予測](https://nlp100.github.io/ja/ch08.html#71-%E5%8D%98%E5%B1%A4%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF%E3%81%AB%E3%82%88%E3%82%8B%E4%BA%88%E6%B8%AC)\n",
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol{y}}_1 = {\\rm softmax}(\\boldsymbol{x}_1 W), \\\\\n",
    "\\hat{Y} = {\\rm softmax}(X_{[1:4]} W)\n",
    "$$\n",
    "\n",
    "ただし，${\\rm softmax}$はソフトマックス関数，$X_{[1:4]} \\in \\mathbb{R}^{4 \\times d}$は特徴ベクトル$\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\boldsymbol{x}_3, \\boldsymbol{x}_4$を縦に並べた行列である．\n",
    "\n",
    "$$\n",
    "X_{[1:4]} = \\begin{pmatrix} \n",
    "  \\boldsymbol{x}_1 \\\\ \n",
    "  \\boldsymbol{x}_2 \\\\ \n",
    "  \\boldsymbol{x}_3 \\\\ \n",
    "  \\boldsymbol{x}_4 \\\\ \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "行列$W \\in \\mathbb{R}^{d \\times L}$は単層ニューラルネットワークの重み行列で，ここではランダムな値で初期化すればよい（問題73以降で学習して求める）．なお，$\\hat{\\boldsymbol{y}}_1 \\in \\mathbb{R}^L$は未学習の行列$W$で事例$x_1$を分類したときに，各カテゴリに属する確率を表すベクトルである．\n",
    "同様に，$\\hat{Y} \\in \\mathbb{R}^{n \\times L}$は，学習データの事例$x_1, x_2, x_3, x_4$について，各カテゴリに属する確率を行列として表現している．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.load(\"Output/Chapter08/x_train.npy\")\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(71)\n",
    "W = np.random.rand(300, len(labels))\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42920103, 0.15615603, 0.20122837, 0.21341457])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_hat = softmax(np.dot(x_train[0], W))\n",
    "y1_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42920103, 0.15615603, 0.20122837, 0.21341457],\n",
       "       [0.32091756, 0.27438051, 0.13843634, 0.26626559],\n",
       "       [0.3609244 , 0.27313285, 0.14794164, 0.21800111],\n",
       "       [0.31986883, 0.21250531, 0.2104081 , 0.25721776]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = softmax(np.dot(x_train[0:4], W), axis=1)\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f6869d9e-85c9-4a2b-a04e-4f80ccafd356"
    }
   },
   "source": [
    "## [72. 損失と勾配の計算](https://nlp100.github.io/ja/ch08.html#72-%E6%90%8D%E5%A4%B1%E3%81%A8%E5%8B%BE%E9%85%8D%E3%81%AE%E8%A8%88%E7%AE%97)\n",
    "学習データの事例$x_1$と事例集合$x_1, x_2, x_3, x_4$に対して，クロスエントロピー損失と，行列$W$に対する勾配を計算せよ．なお，ある事例$x_i$に対して損失は次式で計算される．\n",
    "\n",
    "$$\n",
    "l_i = - \\log [\\mbox{事例}x_i\\mbox{が}y_i\\mbox{に分類される確率}]\n",
    "$$\n",
    "\n",
    "ただし，事例集合に対するクロスエントロピー損失は，その集合に含まれる各事例の損失の平均とする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10684,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.load(\"Output/Chapter08/y_train.npy\")\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8458298750279626"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = -1.0 * np.log(y1_hat[y_train[0]])\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0745000866864252"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = -1.0 * sum([np.log(Y_hat[i][y_train[i]]) for i in range(4)]) / 4.0\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1 = np.dot(x_train[0].reshape(300, 1), (y1_hat - y_train[0]).reshape(1, 4))\n",
    "g1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = np.dot(x_train[0:4].T, Y_hat - y_train[0:4])\n",
    "G.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f6869d9e-85c9-4a2b-a04e-4f80ccafd356"
    }
   },
   "source": [
    "## [73. 確率的勾配降下法による学習](https://nlp100.github.io/ja/ch08.html#73-%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95%E3%81%AB%E3%82%88%E3%82%8B%E5%AD%A6%E7%BF%92)\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列$W$を学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4e1914fdb0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(73)\n",
    "np.random.seed(73)\n",
    "torch.manual_seed(73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsAggregatorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.x_data = torch.from_numpy(x.astype(np.float32))\n",
    "        self.y_data = torch.from_numpy(y.astype(np.int64))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_in = self.x_data[idx]\n",
    "        y = self.y_data[idx]\n",
    "        return x_in, y\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        self.x_data.to(device)\n",
    "        self.y_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network73(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network73, self).__init__()\n",
    "\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            # (batch,300) -> (batch,4)\n",
    "            torch.nn.Linear(in_features=300,\n",
    "                            out_features=4,\n",
    "                            bias=False),\n",
    "        )\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch,300) -> (batch,4)\n",
    "        x = self.fc(x)\n",
    "        return self.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = NewsAggregatorDataset(x_train, y_train)\n",
    "loader_train = torch.utils.data.DataLoader(dataset_train,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)\n",
    "epoch, print_interval, lr = 100, 5, 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5/100,Average Loss:0.423,Accuracy:86.36%\n",
      "Epoch:10/100,Average Loss:0.366,Accuracy:88.28%\n",
      "Epoch:15/100,Average Loss:0.343,Accuracy:88.74%\n",
      "Epoch:20/100,Average Loss:0.329,Accuracy:89.17%\n",
      "Epoch:25/100,Average Loss:0.320,Accuracy:89.50%\n",
      "Epoch:30/100,Average Loss:0.313,Accuracy:89.74%\n",
      "Epoch:35/100,Average Loss:0.307,Accuracy:89.84%\n",
      "Epoch:40/100,Average Loss:0.302,Accuracy:89.96%\n",
      "Epoch:45/100,Average Loss:0.299,Accuracy:90.06%\n",
      "Epoch:50/100,Average Loss:0.295,Accuracy:90.15%\n",
      "Epoch:55/100,Average Loss:0.292,Accuracy:90.23%\n",
      "Epoch:60/100,Average Loss:0.290,Accuracy:90.30%\n",
      "Epoch:65/100,Average Loss:0.287,Accuracy:90.34%\n",
      "Epoch:70/100,Average Loss:0.285,Accuracy:90.35%\n",
      "Epoch:75/100,Average Loss:0.284,Accuracy:90.34%\n",
      "Epoch:80/100,Average Loss:0.282,Accuracy:90.43%\n",
      "Epoch:85/100,Average Loss:0.280,Accuracy:90.51%\n",
      "Epoch:90/100,Average Loss:0.279,Accuracy:90.44%\n",
      "Epoch:95/100,Average Loss:0.278,Accuracy:90.57%\n",
      "Epoch:100/100,Average Loss:0.277,Accuracy:90.57%\n"
     ]
    }
   ],
   "source": [
    "network = Network73()\n",
    "\n",
    "for e in range(1, epoch+1):\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=lr)\n",
    "    loss_sum = 0.0\n",
    "    correct_sum = 0\n",
    "\n",
    "    for x_in, y in loader_train:\n",
    "        network.zero_grad()\n",
    "        x_out = network(x_in)\n",
    "        loss = criterion(x_out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % print_interval == 0:\n",
    "            loss_sum += loss.item() * x_in.shape[0]\n",
    "            x_pred = torch.argmax(x_out, dim=1)\n",
    "            correct_sum += int(torch.sum(x_pred==y))\n",
    "\n",
    "    if e % print_interval == 0:\n",
    "        loss_ave = loss_sum / len(dataset_train)\n",
    "        accuracy = 100.0 * correct_sum / len(dataset_train)\n",
    "        args = (e, epoch, loss_ave, accuracy)\n",
    "        print_str = \"Epoch:%d/%d,Average Loss:%.3f,Accuracy:%.2f%%\"\n",
    "        print(print_str % args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f6869d9e-85c9-4a2b-a04e-4f80ccafd356"
    }
   },
   "source": [
    "## [74. 正解率の計測](https://nlp100.github.io/ja/ch08.html#74-%E6%AD%A3%E8%A7%A3%E7%8E%87%E3%81%AE%E8%A8%88%E6%B8%AC)\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:90.57%\n"
     ]
    }
   ],
   "source": [
    "correct_sum = 0\n",
    "\n",
    "for x_in, y in loader_train:\n",
    "    x_out = network(x_in)\n",
    "    x_pred = torch.argmax(x_out, dim=1)\n",
    "    correct_sum += int(torch.sum(x_pred==y))\n",
    "\n",
    "accuracy = 100.0 * correct_sum / len(dataset_train)\n",
    "print(\"Training Accuracy:%.2f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = np.load(\"Output/Chapter08/x_valid.npy\")\n",
    "y_valid = np.load(\"Output/Chapter08/y_valid.npy\")\n",
    "dataset_valid = NewsAggregatorDataset(x_valid, y_valid)\n",
    "loader_valid = torch.utils.data.DataLoader(dataset_valid,\n",
    "                                           batch_size=1,\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:89.97%\n"
     ]
    }
   ],
   "source": [
    "correct_sum = 0\n",
    "\n",
    "for x_in, y in loader_valid:\n",
    "    x_out = network(x_in)\n",
    "    x_pred = torch.argmax(x_out, dim=1)\n",
    "    correct_sum += int(torch.sum(x_pred==y))\n",
    "\n",
    "accuracy = 100.0 * correct_sum / len(dataset_valid)\n",
    "print(\"Validation Accuracy:%.2f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f6869d9e-85c9-4a2b-a04e-4f80ccafd356"
    }
   },
   "source": [
    "## [75. 損失と正解率のプロット](https://nlp100.github.io/ja/ch08.html#75-%E6%90%8D%E5%A4%B1%E3%81%A8%E6%AD%A3%E8%A7%A3%E7%8E%87%E3%81%AE%E3%83%97%E3%83%AD%E3%83%83%E3%83%88)\n",
    "問題73のコードを改変し，各エポックのパラメータ更新が完了するたびに，訓練データでの損失，正解率，検証データでの損失，正解率をグラフにプロットし，学習の進捗状況を確認できるようにせよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f6869d9e-85c9-4a2b-a04e-4f80ccafd356"
    }
   },
   "source": [
    "## [76. チェックポイント](https://nlp100.github.io/ja/ch08.html#76-%E3%83%81%E3%82%A7%E3%83%83%E3%82%AF%E3%83%9D%E3%82%A4%E3%83%B3%E3%83%88)\n",
    "問題75のコードを改変し，各エポックのパラメータ更新が完了するたびに，チェックポイント（学習途中のパラメータ（重み行列など）の値や最適化アルゴリズムの内部状態）をファイルに書き出せ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f6869d9e-85c9-4a2b-a04e-4f80ccafd356"
    }
   },
   "source": [
    "## [77. ミニバッチ化](https://nlp100.github.io/ja/ch08.html#77-%E3%83%9F%E3%83%8B%E3%83%90%E3%83%83%E3%83%81%E5%8C%96)\n",
    "問題76のコードを改変し，$B$事例ごとに損失・勾配を計算し，行列$W$の値を更新せよ（ミニバッチ化）．$B$の値を$1, 2, 4, 8, \\dots$と変化させながら，1エポックの学習に要する時間を比較せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f6869d9e-85c9-4a2b-a04e-4f80ccafd356"
    }
   },
   "source": [
    "## [78. GPU上での学習](https://nlp100.github.io/ja/ch08.html#78-gpu%E4%B8%8A%E3%81%A7%E3%81%AE%E5%AD%A6%E7%BF%92)\n",
    "問題77のコードを改変し，GPU上で学習を実行せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "aa2217da-12ca-413b-a34b-ca3dd83868aa"
    }
   },
   "source": [
    "## [79. 多層ニューラルネットワーク](https://nlp100.github.io/ja/ch08.html#79-%E5%A4%9A%E5%B1%A4%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF)\n",
    "問題78のコードを改変し，バイアス項の導入や多層化など，ニューラルネットワークの形状を変更しながら，高性能なカテゴリ分類器を構築せよ．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
